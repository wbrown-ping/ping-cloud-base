## This is a Prometheus alerting rules for the logging
---

groups:
- name: elastic-stack-logging
  rules:
  - alert: FluentBit dropping records
    expr: sum(increase(fluentbit_output_dropped_records_total[1m])) by (name) > 10
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: FluentBit dropping records for output {{ $labels.name }}
      description: "A FluentBit outputs dropping records.\nVALUE = {{ $value }}\nLABELS = {{ $labels }}"
      runbook: "https://pingidentity.atlassian.net/l/cp/1LNbK1h1"

  - alert: FluentBit retries is growing
    expr: sum(increase(fluentbit_output_retries_total[1m])) by (name) > 50
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: FluentBit has many retries in the output {{ $labels.name }}
      description: "A FluentBit has many retries in the output. Possible {{ $labels.name }} output is down.\n  VALUE = {{ $value }}\nLABELS = {{ $labels }}"
      runbook: "https://pingidentity.atlassian.net/l/cp/vQCAwBPs"

  - alert: Logstash pipeline is growing
    expr: delta(logstash_node_queue_events[1m]) > 10000
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: Logstash pipeline {{ $labels.pipeline }} is growing on {{ $labels.instance }}.
      description: "A Logstash pipeline is growing. Possible {{ $labels.pipeline }} is overloaded or logs spike.\nVALUE = {{ $value }}\nLABELS = {{ $labels }}"
      runbook: "https://pingidentity.atlassian.net/l/cp/fKEd3bpy"

  - alert: Logstash pipeline backlog alert
    expr: logstash_node_queue_events > 1000000
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: Logstash Queue Backlog Alert for {{ $labels.pipeline }} on {{ $labels.instance }}
      description: "This alert is triggered when the length of the Logstash event queue for {{ $labels.pipeline }} on {{ $labels.instance }} exceeds a certain threshold.\nVALUE = {{ $value }}\nLABELS = {{ $labels }}"
      runbook: "https://pingidentity.atlassian.net/l/cp/ARLdMfXF"

  - alert: Logstash pipeline backlog alert
    expr: logstash_node_queue_events > 1000000
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: Logstash Queue Backlog Alert for {{ $labels.pipeline }} on {{ $labels.instance }}
      description: "This alert is triggered when the length of the Logstash event queue for {{ $labels.pipeline }} on {{ $labels.instance }} exceeds a certain threshold.\nInvestigate the cause of the backlog for the specified pipeline on the specified instance.\nPossible reason is logstash or Elasticsearch performance issue.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

  - alert: Logstash has no outgoing events
    expr: sum(increase(logstash_node_pipeline_events_out_total{pipeline!="alerts"}[1m])) by (pipeline) == 0
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: Logstash pipeline {{ $labels.pipeline }} didn't produce any outgoing events.
      description: "An outgoing Logstash pipeline {{ $labels.pipeline }} is dead.\nVALUE = {{ $value }}\nLABELS = {{ $labels }}"
      runbook: "https://pingidentity.atlassian.net/l/cp/2Y87TfgA"

  - alert: Logstash has no incoming events
    expr: sum(increase(logstash_node_pipeline_events_in_total{pipeline!="alerts"}[1m])) by (pipeline) == 0
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: Logstash pipeline {{ $labels.pipeline }} didn't recieve any incoming events.
      description: "An incoming Logstash pipeline {{ $labels.pipeline }} is dead.\nVALUE = {{ $value }}\nLABELS = {{ $labels }}"
      runbook: "https://pingidentity.atlassian.net/l/cp/RvB39RpB"

  - alert: Logstash pods count 0
    expr: sum(logstash_node_up) == 0 or absent(logstash_node_up) == 1
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: Running logstash pods is count 0
      description: "Running logstash pods is 0. Looks like they're died or didn't return any metrics."
      runbook: "https://pingidentity.atlassian.net/l/cp/Rub1Qv6f"

- name: opensearch.rules
  rules:
  - expr: |
      rate(opensearch_threadpool_threads_count{name="bulk", type="rejected"}[2m])
    record: bulk:rejected_requests:rate2m
  - expr: |
      rate(opensearch_threadpool_threads_count{name="bulk", type="completed"}[2m])
    record: bulk:completed_requests:rate2m
  - expr: |
      sum by (cluster, instance, node) (bulk:rejected_requests:rate2m) / on (cluster, instance, node) (bulk:completed_requests:rate2m)
    record: bulk:reject_ratio:rate2m

- name: opensearch.alerts
  rules:
  - alert: OpenSearchClusterNotHealthy
    annotations:
      description: "Cluster {{ $labels.cluster }} health status has been RED for at least 2m. Cluster does not accept writes, shards may be missing or master node hasn't been elected yet."
      summary: "Cluster health status is RED"
      runbook: "https://pingidentity.atlassian.net/l/cp/s1hJ29Go"
    expr: |
      sum by (cluster) (opensearch_cluster_status == 2)
    for: 2m
    labels:
      severity: critical
  - alert: OpenSearchClusterNotHealthy
    annotations:
      description: "Cluster {{ $labels.cluster }} health status has been YELLOW for at least 20m. Some shard replicas are not allocated."
      summary: "Cluster health status is YELLOW"
      runbook: "https://pingidentity.atlassian.net/l/cp/s1hJ29Go"
    expr: |
      sum by (cluster) (opensearch_cluster_status == 1)
    for: 20m
    labels:
      severity: warning
  - alert: OpenSearchBulkRequestsRejectionJumps
    annotations:
      description: "High Bulk Rejection Ratio at {{ $labels.node }} node in {{ $labels.cluster }} cluster. This node may not be keeping up with the indexing speed."
      summary: "High Bulk Rejection Ratio - {{ $value }}%"
      runbook: "https://pingidentity.atlassian.net/l/cp/9HSdgGTX"
    expr: |
      round( bulk:reject_ratio:rate2m * 100, 0.001 ) > 5
    for: 10m
    labels:
      severity: warning
  - alert: OpenSearchNodeDiskWatermarkReached
    annotations:
      description: "Disk Low Watermark Reached at {{ $labels.node }} node in {{ $labels.cluster }} cluster. Shards can not be allocated to this node anymore. You should consider adding more disk to the node."
      summary: "Disk Low Watermark Reached - disk saturation is {{ $value }}%"
      runbook: "https://pingidentity.atlassian.net/l/cp/ceuvTPL0"
    expr: |
      sum by (cluster, instance, node) (
        round(
          (1 - (
            opensearch_fs_path_available_bytes /
            opensearch_fs_path_total_bytes
          )
        ) * 100, 0.001)
      ) > 85
    for: 5m
    labels:
      severity: alert
  - alert: OpenSearchNodeDiskWatermarkReached
    annotations:
      description: "Disk High Watermark Reached at {{ $labels.node }} node in {{ $labels.cluster }} cluster. Some shards will be re-allocated to different nodes if possible. Make sure more disk space is added to the node or drop old indices allocated to this node."
      summary: "Disk High Watermark Reached - disk saturation is {{ $value }}%"
      runbook: "https://pingidentity.atlassian.net/l/cp/ceuvTPL0"
    expr: |
      sum by (cluster, instance, node) (
        round(
          (1 - (
            opensearch_fs_path_available_bytes /
            opensearch_fs_path_total_bytes
          )
        ) * 100, 0.001)
      ) > 90
    for: 5m
    labels:
      severity: high
  - alert: OpenSearchJVMHeapUseHigh
    annotations:
      description: "JVM Heap usage on the node {{ $labels.node }} in {{ $labels.cluster }} cluster is {{ $value }}%."
      summary: "JVM Heap usage on the node is high"
      runbook: "https://pingidentity.atlassian.net/l/cp/KMH9013e"
    expr: |
      sum by (cluster, instance, node) (opensearch_jvm_mem_heap_used_percent) > 75
    for: 10m
    labels:
      severity: alert
  - alert: OpenSearchHostSystemCPUHigh
    annotations:
      description: "System CPU usage on the node {{ $labels.node }} in {{ $labels.cluster }} cluster is {{ $value }}%"
      summary: "System CPU usage is high"
      runbook: "https://pingidentity.atlassian.net/l/cp/8qTf1caL"
    expr: |
      sum by (cluster, instance, node) (opensearch_os_cpu_percent) > 90
    for: 1m
    labels:
      severity: alert
  - alert: OpenSearchProcessCPUHigh
    annotations:
      description: "OSE process CPU usage on the node {{ $labels.node }} in {{ $labels.cluster }} cluster is {{ $value }}%"
      summary: "OSE process CPU usage is high"
      runbook: "https://pingidentity.atlassian.net/l/cp/8qTf1caL"
    expr: |
      sum by (cluster, instance, node) (opensearch_process_cpu_percent) > 90
    for: 1m
    labels:
      severity: alert